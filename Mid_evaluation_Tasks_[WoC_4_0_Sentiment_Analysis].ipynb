{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mid - evaluation Tasks [WoC 4.0: Sentiment Analysis].ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-reqs"
      ],
      "metadata": {
        "id": "1JcCWwVD3xXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mmh3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Vv791_wv58d",
        "outputId": "87e0ea36-d010-493c-e6fa-7bd981fc5c4e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mmh3\n",
            "  Downloading mmh3-3.0.0-cp37-cp37m-manylinux2010_x86_64.whl (50 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████▍                         | 10 kB 25.3 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 20 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 30 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 40 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 50 kB 3.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: mmh3\n",
            "Successfully installed mmh3-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0KC5gVN0vUr1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import mmh3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1"
      ],
      "metadata": {
        "id": "EQFRAh6Pv_sa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopwords Removal"
      ],
      "metadata": {
        "id": "nfhvKV57GLjq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " In this exercise you have to remove stopwords from a given **Initial** text and convert the text into the **Final** text given to you \n",
        "\n",
        "*Note - Please only write your code where it is mentioned to and do not change anything else*"
      ],
      "metadata": {
        "id": "wcklv4wFGUZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Intial text**\n",
        "\n",
        "> Sentiment analysis is the process of using natural language processing , text analysis , and statistics to analyze customer sentiment .\n",
        "\n",
        "\n",
        "**Final text**\n",
        "\n",
        "> Sentiment is process using natural language processing , text , statistics analyze customer ."
      ],
      "metadata": {
        "id": "6TMIz90TGk__"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VvSIL7XR_0AC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "813a0283-fe28-411e-f486-a58b38541374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize your text"
      ],
      "metadata": {
        "id": "DRF0hW7uGHGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Sentiment analysis is the process of using natural language processing , text analysis , and statistics to analyze customer sentiment .\""
      ],
      "metadata": {
        "id": "lsOP7BsC_8oo"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_tokens = word_tokenize(text)"
      ],
      "metadata": {
        "id": "LoqIzFvdAQD3"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_tokens)"
      ],
      "metadata": {
        "id": "yxQRfHDQAcrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a984fe8e-2ec0-41f3-c81f-2b890a9dc736"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sentiment', 'analysis', 'is', 'the', 'process', 'of', 'using', 'natural', 'language', 'processing', ',', 'text', 'analysis', ',', 'and', 'statistics', 'to', 'analyze', 'customer', 'sentiment', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Expected output*\n",
        "\n",
        "['Sentiment', 'analysis', 'is', 'the', 'process', 'of', 'using', 'natural', 'language', 'processing', ',', 'text', 'analysis', ',', 'and', 'statistics', 'to', 'analyze', 'customer', 'sentiment', '.']"
      ],
      "metadata": {
        "id": "XVApTuyhHM_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the stopwords list"
      ],
      "metadata": {
        "id": "0ZIwl7E1HfXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopword_list = stopwords.words('english') # now stopword_list will be appended by already known common stopwords which can be seen by prinitng the list below #"
      ],
      "metadata": {
        "id": "gl9hYgbGEgVg"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopword_list)"
      ],
      "metadata": {
        "id": "bPiEJ434K0PU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bdc9955-a27c-4921-bf85-06b594ffcea8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Append and Remove some stopwords from the list such that after removing stopwords from intial text using stopwords_list you can get final text\n",
        "\n",
        "# ...\n",
        "#code\n",
        "# ... "
      ],
      "metadata": {
        "id": "Q3zBIboZH-ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Remove all tokens from text_tokens which are also present in stopword_list and store remaining tokens in sw_filtered_text#\n",
        "\n",
        " #...\n",
        " # write your code here (Hint - create a list named sw_filtered_text fulfilling above criteria)\n",
        " \n",
        " #sw_filtered_text=list(set(text_tokens).difference(set(stopword_list)))\n",
        " sw_filtered_text=text_tokens\n",
        " for i in text_tokens[:]:\n",
        "        if i in stopword_list:\n",
        "            sw_filtered_text.remove(i)\n",
        " #.."
      ],
      "metadata": {
        "id": "MFh_nGN1DRpy"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sw_filtered_text)"
      ],
      "metadata": {
        "id": "56k3icocDR74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a506d7a9-6ca7-406e-874f-be6f02d0deef"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sentiment', 'analysis', 'process', 'using', 'natural', 'language', 'processing', ',', 'text', 'analysis', ',', 'statistics', 'analyze', 'customer', 'sentiment', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Expected output*\n",
        "\n",
        "['Sentiment', 'is', 'process', 'using', 'natural', 'language', 'processing', ',', 'text', ',', 'statistics', 'analyze', 'customer', '.']"
      ],
      "metadata": {
        "id": "aoI798IQJjRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output"
      ],
      "metadata": {
        "id": "nd80wGJQAewo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sw_filtered_text = (\" \").join(sw_filtered_text)\n",
        "print(sw_filtered_text)\n",
        "\n",
        "\n",
        "# The above code will join all the tokens in the final list now check whether your output with the final text given to you or not."
      ],
      "metadata": {
        "id": "P6MRAcAFErFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc03183c-4688-4da8-ca93-436d32a11a7c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment analysis process using natural language processing , text analysis , statistics analyze customer sentiment .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2"
      ],
      "metadata": {
        "id": "IKT900gTwFbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming"
      ],
      "metadata": {
        "id": "jGMPQJMf4oZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise, with the given input, produce the given output by stemming the previously tokenized words.\n",
        "\n",
        "*Note - Write your code only where it is mentioned to and keep everything else unchanged.*"
      ],
      "metadata": {
        "id": "cryVbEXp4oZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Input**\n",
        "> ['Constantly', 'concentrating', 'nearby', 'object', 'may', 'lead', 'one', 'feel', 'strain' 'eyes']\n",
        "\n",
        "\n",
        "\n",
        "**Output**\n",
        "> ['Constant', 'concentrat', 'near', 'object', 'may', 'lead', 'on', 'feel', 'strain' 'ey']"
      ],
      "metadata": {
        "id": "BD1VxLUF4oZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def strip_end(text, suffix):\n",
        "    if suffix and text.endswith(suffix):\n",
        "        return text[:-len(suffix)]\n",
        "    return text\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "stemmed_tockens = ['Constantly', 'concentrating', 'nearby', 'object', 'may', 'lead', 'one', 'feel', 'strain', 'eyes']\n",
        "#stemmed_tockens = word_tokenize(sentence)\n",
        "#for i in range(len(stemmed_tockens)):\n",
        "#   stemmed_tockens[i]=ps.stem(stemmed_tockens[i])\n",
        "#stemmed_tockens = # Your code here\n",
        "\n",
        "for i in range(len(stemmed_tockens)):\n",
        "  stemmed_tockens[i]=strip_end(stemmed_tockens[i],\"ly\")\n",
        "  stemmed_tockens[i]=strip_end(stemmed_tockens[i],\"ing\")\n",
        "  stemmed_tockens[i]=strip_end(stemmed_tockens[i],\"by\")\n",
        "  stemmed_tockens[i]=strip_end(stemmed_tockens[i],\"es\")\n",
        "  stemmed_tockens[i]=strip_end(stemmed_tockens[i],\"s\")\n",
        "  "
      ],
      "metadata": {
        "id": "ljE4xRlO-eOc"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final output\n",
        "stemmed_tockens"
      ],
      "metadata": {
        "id": "BInjGAZw-qSC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8da88e24-539e-465e-bf77-06e68220b12d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Constant',\n",
              " 'concentrat',\n",
              " 'near',\n",
              " 'object',\n",
              " 'may',\n",
              " 'lead',\n",
              " 'one',\n",
              " 'feel',\n",
              " 'strain',\n",
              " 'ey']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questionnaire:\n",
        "1. Have you written an entire function for stemming? - YES\n",
        "2. Have you used any pre-built library for this task? - YES\n",
        "3. Name the library used (write NA if not used): PorterStemmer"
      ],
      "metadata": {
        "id": "KNKHhvCW-xVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3"
      ],
      "metadata": {
        "id": "IOMr50QBwHQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Hashing Trick"
      ],
      "metadata": {
        "id": "FDCBJRLvwJbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The conventional Bag of Words method involves formation of vocabulary out of all the available tokens in the given text and later, building features out of the vocabulary using frequencies of tokens or just their presence (1s or 0s). Feature hashing or the hashing trick helps overcome some of the major limitations of conventional Bag of Words like growing vocabulary with growing dataset size i.e. dimensionality reduction or dodging crafty corner cases. \n",
        "\n",
        "For some further details on Feature hashing, refer to these links:\n",
        "1. https://en.wikipedia.org/wiki/Feature_hashing\n",
        "2. https://medium.com/value-stream-design/introducing-one-of-the-best-hacks-in-machine-learning-the-hashing-trick-bf6a9c8af18f\n",
        "\n",
        "For this Task, given some arbitrary data in a DataFrame, implement Feature hashing manually using MurmurHash3 (using mmh3 module imported at the beginning) to hash each word and then modulo to build a feature vector as shown in the below example:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Example:\n",
        "\n",
        "**Dataset**\n",
        "\n",
        "| Text      | Label |\n",
        "| --------- | ----- |\n",
        "| Loved the service      | 1       |\n",
        "| Terrible food   | 0        |\n",
        "\n",
        "Using conventional bag of words, we would require 5 features for 5 unique words in both the sentences (loved, the, service, terrible, food).\n",
        "\n",
        "Let us use feature hashing such that output feature vector has 4 features (use modulo 4 after hashing)\n",
        "\n",
        "Sentence 1: Loved the service\n",
        "\n",
        "| Words      | Hash | Hash modulo 4 |\n",
        "| --------- | ----- | ------------- |\n",
        "| loved | 2334787929 | 1 |\n",
        "| the   | 3162218338 | 2 |\n",
        "| service | 3640852848 | 0 |\n",
        "\n",
        "Sentence 2: Terrible food\n",
        "\n",
        "| Words      | Hash | Hash modulo 4 |\n",
        "| --------- | ----- | ------------- |\n",
        "| terrible | 2800746226 | 2 |\n",
        "| food   | 2122679414 | 2 |\n",
        "\n",
        "Final feature vector:\n",
        "\n",
        "| Sentence      | Hash Feature 0 | Hash Feature 1 | Hash Feature 2 | Hash Feature 3 |\n",
        "| --------- | ----- | ------------- | ----- | ----- |\n",
        "| Loved the service | 1 | 1 | 1 | 0 |\n",
        "| Terrible food | 0 | 0 | 2 | 0 |\n",
        "\n",
        "Thus the output should be:\n",
        "\n",
        "\n",
        "```\n",
        "sample_output = [1, 1, 1, 0\n",
        "                 0, 0, 2, 0]\n",
        "```\n",
        "\n",
        "For the dataset given in the problem: \n",
        "\n",
        "- Required features: 1000\n",
        "\n",
        "- Hash function: mmh3 (must use unsigned hash)"
      ],
      "metadata": {
        "id": "Loy-mazWySll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE: You should only use Pandas, NumPy and mmh3 for this task, if any other library is used, your submission will not be considered.**"
      ],
      "metadata": {
        "id": "DaAU0AMn7Jxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/data_BoW.csv')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "FiZm2ubU7XTL",
        "outputId": "186275a3-2a9e-4429-fdb8-b5bcd2fdfcbe"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8d115c7d-f0b8-4c33-afba-3da2378afcf0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>label_num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>subject enron methanol meter this is a follow ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>subject hpl nom for january see attached file ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>subject neon retreat ho ho ho we re around to ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>subject photoshop windows office cheap main tr...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>subject re indian springs this deal is to book...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1230</th>\n",
              "      <td>1230</td>\n",
              "      <td>subject offshore pharmacy online pharmacy prov...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1231</th>\n",
              "      <td>1231</td>\n",
              "      <td>subject don t buy viia gra so try our revoluti...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1232</th>\n",
              "      <td>1232</td>\n",
              "      <td>subject maverick microcap stock we want to con...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1233</th>\n",
              "      <td>1233</td>\n",
              "      <td>subject reviewers fyi don t freak at the of re...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1234</th>\n",
              "      <td>1234</td>\n",
              "      <td>subject urgent message mr francis oma attn dir...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1235 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d115c7d-f0b8-4c33-afba-3da2378afcf0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8d115c7d-f0b8-4c33-afba-3da2378afcf0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8d115c7d-f0b8-4c33-afba-3da2378afcf0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Unnamed: 0                                         text_clean  label_num\n",
              "0              0  subject enron methanol meter this is a follow ...        0.0\n",
              "1              1  subject hpl nom for january see attached file ...        0.0\n",
              "2              2  subject neon retreat ho ho ho we re around to ...        0.0\n",
              "3              3  subject photoshop windows office cheap main tr...        1.0\n",
              "4              4  subject re indian springs this deal is to book...        0.0\n",
              "...          ...                                                ...        ...\n",
              "1230        1230  subject offshore pharmacy online pharmacy prov...        1.0\n",
              "1231        1231  subject don t buy viia gra so try our revoluti...        1.0\n",
              "1232        1232  subject maverick microcap stock we want to con...        1.0\n",
              "1233        1233  subject reviewers fyi don t freak at the of re...        0.0\n",
              "1234        1234  subject urgent message mr francis oma attn dir...        NaN\n",
              "\n",
              "[1235 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=df[\"text_clean\"]\n",
        "#he = mmh3.hash(1,signed=False)\n",
        "df.apply(lambda x: mmh3.hash(\"foo\",signed=False), axis = 1)\n",
        "#data = he.transform(text)\n",
        "#print(data.info())"
      ],
      "metadata": {
        "id": "SwNiRpMdD-5L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae13544-cf0a-4eb7-dc74-ab0bdc90aa09"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       4138058784\n",
              "1       4138058784\n",
              "2       4138058784\n",
              "3       4138058784\n",
              "4       4138058784\n",
              "           ...    \n",
              "1230    4138058784\n",
              "1231    4138058784\n",
              "1232    4138058784\n",
              "1233    4138058784\n",
              "1234    4138058784\n",
              "Length: 1235, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    }
  ]
}